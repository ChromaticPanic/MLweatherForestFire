{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv\n",
    "# %pip install seaborn\n",
    "# %pip install tensorflow_data_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Red\\AppData\\Local\\Temp\\ipykernel_327632\\2532399237.py:3: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeos as pg\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_data_validation as tfdv\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "# from datetime import datetime\n",
    "# from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely import wkt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines adjust the granularity of reporting.\n",
    "#pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "os.chdir('F:\\\\Uni Files\\\\4710\\\\4710 Project\\\\MLweatherForestFire')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPDfromPD(df: pd.DataFrame, geomCol: str, crs: str = \"EPSG:3978\") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert a pandas dataframe to a geopandas dataframe\n",
    "    :param df: pandas dataframe\n",
    "    :param geomCol: name of the geometry column\n",
    "    :param crs: coordinate reference system\n",
    "    :return: geopandas dataframe\n",
    "    \"\"\"\n",
    "    if 'geom' in df.columns:\n",
    "        df.rename(columns={'geom': 'geometry'}, inplace=True)\n",
    "\n",
    "    df[geomCol] = df[geomCol].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geomCol, crs=crs)\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireWeatherTable = \"Data/FinalFeature.csv\"\n",
    "dfFireWeather = pd.read_csv(fireWeatherTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>SIZE_HA</th>\n",
       "      <th>SIZE_HA_BIN</th>\n",
       "      <th>ELEVATIONM</th>\n",
       "      <th>DIST_TO_WATER</th>\n",
       "      <th>DAYW</th>\n",
       "      <th>MAXTEMP</th>\n",
       "      <th>MEANHUMIDITY</th>\n",
       "      <th>MEANWINDSPEED</th>\n",
       "      <th>MAXWINDSPEED</th>\n",
       "      <th>TOTALPRECIP</th>\n",
       "      <th>LONG</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LONGBIN</th>\n",
       "      <th>LATBIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1143.2</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>16.7</td>\n",
       "      <td>10705.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>742.7</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>61.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5375901.9</td>\n",
       "      <td>2375607.2</td>\n",
       "      <td>365.0</td>\n",
       "      <td>384.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1439.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29415.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>513.4</td>\n",
       "      <td>4094.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>139093.0</td>\n",
       "      <td>204885.0</td>\n",
       "      <td>111.1</td>\n",
       "      <td>97.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>283.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>201.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4918132.3</td>\n",
       "      <td>1567321.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>522.8</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>704.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1114.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5267895.2</td>\n",
       "      <td>2246671.4</td>\n",
       "      <td>278.5</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>753.5</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2070.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2467.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5379082.5</td>\n",
       "      <td>2392909.7</td>\n",
       "      <td>367.5</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1539.2</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7250.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1161.0</td>\n",
       "      <td>5318.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>68.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5493470.9</td>\n",
       "      <td>2524069.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10650.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>476376.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>29418.8</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>96.9</td>\n",
       "      <td>30.1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>5632110.7</td>\n",
       "      <td>2763468.2</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ENTRYID   YEAR  MONTH   DAY  SIZE_HA  SIZE_HA_BIN  ELEVATIONM  \\\n",
       "count    836.0  836.0  836.0 836.0    836.0        836.0       836.0   \n",
       "mean    1143.2 2009.5    6.4  16.7  10705.3          1.7       742.7   \n",
       "std     1439.2    5.3    1.1   9.0  29415.9          1.1       513.4   \n",
       "min      283.0 2000.0    1.0   2.0    201.8          0.0       225.0   \n",
       "25%      522.8 2005.0    6.0   8.0    704.4          1.0       434.0   \n",
       "50%      753.5 2010.0    6.0  17.0   2070.2          2.0       496.0   \n",
       "75%     1539.2 2015.0    7.0  25.0   7250.0          3.0      1161.0   \n",
       "max    10650.0 2020.0   10.0  31.0 476376.8          3.0      3668.0   \n",
       "\n",
       "       DIST_TO_WATER  DAYW  MAXTEMP  MEANHUMIDITY  MEANWINDSPEED  \\\n",
       "count          836.0 836.0    836.0         836.0          836.0   \n",
       "mean          3881.6  15.7     23.1          61.2            8.3   \n",
       "std           4094.2   9.0      6.4          12.1            3.4   \n",
       "min              0.0   1.0     -6.2          28.0            2.1   \n",
       "25%           1114.6   7.0     21.1          53.0            5.9   \n",
       "50%           2467.7  16.0     24.4          61.0            7.5   \n",
       "75%           5318.9  24.0     26.9          68.6           10.2   \n",
       "max          29418.8  30.0     34.9          96.9           30.1   \n",
       "\n",
       "       MAXWINDSPEED  TOTALPRECIP      LONG       LAT  LONGBIN  LATBIN  \n",
       "count         836.0        836.0     836.0     836.0    836.0   836.0  \n",
       "mean           16.2          0.2 5375901.9 2375607.2    365.0   384.7  \n",
       "std             5.9          1.5  139093.0  204885.0    111.1    97.6  \n",
       "min             4.0          0.0 4918132.3 1567321.0      0.0     0.0  \n",
       "25%            11.0          0.0 5267895.2 2246671.4    278.5   323.0  \n",
       "50%            16.0          0.0 5379082.5 2392909.7    367.5   393.0  \n",
       "75%            20.0          0.0 5493470.9 2524069.0    459.0   455.0  \n",
       "max            37.0         20.8 5632110.7 2763468.2    569.0   569.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEval = dfFireWeather\n",
    "dfEval.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L    209\n",
       "M    209\n",
       "H    209\n",
       "E    209\n",
       "Name: SIZE_HA_BIN, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log transform SIZE_HA\n",
    "dfEval['SIZE_HA'] = np.log(dfEval['SIZE_HA'])\n",
    "\n",
    "# binning SIZE_HA into 4 categories L M H E in SIZE_HA_BIN\n",
    "dfEval['SIZE_HA_BIN'] = pd.qcut(dfEval['SIZE_HA'], 4, labels=['L', 'M', 'H', 'E'])\n",
    "\n",
    "# count the number of fires in each SIZE_HA_BIN\n",
    "dfEval['SIZE_HA_BIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ENTRYID   YEAR  MONTH   DAY  SIZE_HA  ELEVATIONM  DIST_TO_WATER  DAYW  \\\n",
      "count    836.0  836.0  836.0 836.0    836.0       836.0          836.0 836.0   \n",
      "mean    1143.2 2009.5    6.4  16.7      7.8       742.7         3881.6  15.7   \n",
      "std     1439.2    5.3    1.1   9.0      1.6       513.4         4094.2   9.0   \n",
      "min      283.0 2000.0    1.0   2.0      5.3       225.0            0.0   1.0   \n",
      "25%      522.8 2005.0    6.0   8.0      6.6       434.0         1114.6   7.0   \n",
      "50%      753.5 2010.0    6.0  17.0      7.6       496.0         2467.7  16.0   \n",
      "75%     1539.2 2015.0    7.0  25.0      8.9      1161.0         5318.9  24.0   \n",
      "max    10650.0 2020.0   10.0  31.0     13.1      3668.0        29418.8  30.0   \n",
      "\n",
      "       MAXTEMP  MEANHUMIDITY  MEANWINDSPEED  MAXWINDSPEED  TOTALPRECIP  \\\n",
      "count    836.0         836.0          836.0         836.0        836.0   \n",
      "mean      23.1          61.2            8.3          16.2          0.2   \n",
      "std        6.4          12.1            3.4           5.9          1.5   \n",
      "min       -6.2          28.0            2.1           4.0          0.0   \n",
      "25%       21.1          53.0            5.9          11.0          0.0   \n",
      "50%       24.4          61.0            7.5          16.0          0.0   \n",
      "75%       26.9          68.6           10.2          20.0          0.0   \n",
      "max       34.9          96.9           30.1          37.0         20.8   \n",
      "\n",
      "           LONG       LAT  LONGBIN  LATBIN  \n",
      "count     836.0     836.0    836.0   836.0  \n",
      "mean  5375901.9 2375607.2    365.0   384.7  \n",
      "std    139093.0  204885.0    111.1    97.6  \n",
      "min   4918132.3 1567321.0      0.0     0.0  \n",
      "25%   5267895.2 2246671.4    278.5   323.0  \n",
      "50%   5379082.5 2392909.7    367.5   393.0  \n",
      "75%   5493470.9 2524069.0    459.0   455.0  \n",
      "max   5632110.7 2763468.2    569.0   569.0  \n"
     ]
    }
   ],
   "source": [
    "# print rows with nan\n",
    "print(dfEval.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our random selection, run once\n",
    "# randomTrain = \"RandomTrain\"\n",
    "# dfTrain.to_sql(randomTrain, db_push_con, if_exists='replace', index=False)\n",
    "\n",
    "# randomTest = \"RandomTest\"\n",
    "# dfTest.to_sql(randomTest, db_push_con, if_exists='replace', index=False)\n",
    "\n",
    "# randomValidate = \"RandomValidate\"\n",
    "# dfValidate.to_sql(randomValidate, db_push_con, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ENTRYID', 'FIRE_ID', 'FIRENAME', 'YEAR', 'MONTH', 'DAY', 'REP_DATE',\n",
       "       'SIZE_HA', 'SIZE_HA_BIN', 'GEOM', 'ELEVATIONM', 'DIST_TO_WATER',\n",
       "       'CLIMATEID', 'PROVINCECODE', 'DAYW', 'MAXTEMP', 'MEANHUMIDITY',\n",
       "       'MEANWINDSPEED', 'MAXWINDSPEED', 'TOTALPRECIP', 'RAIN', 'LONG', 'LAT',\n",
       "       'LONGBIN', 'LATBIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEval.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6551.109478673032"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of size_ha column\n",
    "dfEval['SIZE_HA'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrim = dfEval.copy(deep=True)\n",
    "\n",
    "# edit this\n",
    "dfTrim = dfTrim.drop(columns={'ENTRYID', 'FIRE_ID', 'FIRENAME', 'GEOM', 'CLIMATEID', 'PROVINCECODE',\n",
    "                              'TOTALPRECIP', 'LONG', 'LAT', 'REP_DATE', 'SIZE_HA'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace size_ha_bin 0 1 2 3 with L M H E\n",
    "\n",
    "# dfTrim['SIZE_HA_BIN'] = dfTrim['SIZE_HA_BIN'].astype(str).replace('0', 'L')\n",
    "# dfTrim['SIZE_HA_BIN'] = dfTrim['SIZE_HA_BIN'].astype(str).replace('1', 'M')\n",
    "# dfTrim['SIZE_HA_BIN'] = dfTrim['SIZE_HA_BIN'].astype(str).replace('2', 'H')\n",
    "# dfTrim['SIZE_HA_BIN'] = dfTrim['SIZE_HA_BIN'].astype(str).replace('3', 'E')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 6 years from 2010-2019 for training\n",
    "dfTrain = dfTrim[dfTrim['YEAR'].isin(\n",
    "    [2010, 2011, 2012, 2013, 2014, 2015, 2016])].drop(columns={'SIZE_HA_BIN'})\n",
    "dfTest = dfTrim[dfTrim['YEAR'].isin([2017, 2018, 2019, 2020])].drop(\n",
    "    columns={'SIZE_HA_BIN'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    # regularize y values using z score\n",
    "    df = (df - mean) / std\n",
    "    # set max value to 3 zscore\n",
    "    df[df > 3] = 3\n",
    "    # set min value to -3 zscore\n",
    "    df[df < -3] = -3\n",
    "\n",
    "    # shift the wole train set to be positive\n",
    "    # df = df + 3\n",
    "\n",
    "    # return dataframe list of means and stds\n",
    "    return df, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainScaled, dfMeans, dfStdevs = scaleData(dfTrain.copy(deep=True))\n",
    "dfTrainScaled['YEAR'] = dfTrain['YEAR']\n",
    "dfTrainScaled['SIZE_HA_BIN'] = dfTrim[dfTrim['YEAR'].isin(\n",
    "    [2010, 2011, 2012, 2013, 2014, 2015, 2016])]['SIZE_HA_BIN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR             357\n",
      "MONTH            357\n",
      "DAY              357\n",
      "ELEVATIONM       357\n",
      "DIST_TO_WATER    357\n",
      "DAYW             357\n",
      "MAXTEMP          357\n",
      "MEANHUMIDITY     357\n",
      "MEANWINDSPEED    357\n",
      "MAXWINDSPEED     357\n",
      "RAIN             357\n",
      "LONGBIN          357\n",
      "LATBIN           357\n",
      "SIZE_HA_BIN      357\n",
      "dtype: int64\n",
      "0\n",
      "YEAR                int64\n",
      "MONTH             float64\n",
      "DAY               float64\n",
      "ELEVATIONM        float64\n",
      "DIST_TO_WATER     float64\n",
      "DAYW              float64\n",
      "MAXTEMP           float64\n",
      "MEANHUMIDITY      float64\n",
      "MEANWINDSPEED     float64\n",
      "MAXWINDSPEED      float64\n",
      "RAIN              float64\n",
      "LONGBIN           float64\n",
      "LATBIN            float64\n",
      "SIZE_HA_BIN      category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dfTrainScaled.count())\n",
    "print(dfTrainScaled.isna().sum().sum())\n",
    "print(dfTrainScaled.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        YEAR  MONTH   DAY  ELEVATIONM  DIST_TO_WATER  DAYW  MAXTEMP  \\\n",
      "count  357.0  357.0 357.0       357.0          357.0 357.0    357.0   \n",
      "mean  2012.9   -0.0  -0.0        -0.0           -0.0  -0.0      0.0   \n",
      "std      2.1    1.0   1.0         1.0            0.9   1.0      0.9   \n",
      "min   2010.0   -2.6  -1.6        -1.0           -0.9  -1.6     -3.0   \n",
      "25%   2011.0   -0.5  -1.0        -0.6           -0.7  -1.0     -0.4   \n",
      "50%   2013.0    0.5   0.1        -0.5           -0.4   0.1      0.2   \n",
      "75%   2015.0    0.5   0.9        -0.2            0.3   0.9      0.6   \n",
      "max   2016.0    3.0   1.5         3.0            3.0   1.5      2.1   \n",
      "\n",
      "       MEANHUMIDITY  MEANWINDSPEED  MAXWINDSPEED  RAIN  LONGBIN  LATBIN  \n",
      "count         357.0          357.0         357.0 357.0    357.0   357.0  \n",
      "mean           -0.0           -0.0          -0.0   0.0     -0.0     0.0  \n",
      "std             1.0            0.9           1.0   1.0      1.0     1.0  \n",
      "min            -2.8           -1.6          -1.8  -0.5     -2.5    -3.0  \n",
      "25%            -0.6           -0.6          -0.7  -0.5     -0.8    -0.7  \n",
      "50%             0.0           -0.2           0.0  -0.5     -0.1     0.0  \n",
      "75%             0.6            0.5           0.8  -0.5      0.9     0.7  \n",
      "max             2.7            3.0           3.0   2.1      1.9     2.1  \n"
     ]
    }
   ],
   "source": [
    "print(dfTrainScaled.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1  # SVM regularization parameter\n",
    "\n",
    "model1 = svm.SVC(kernel=\"linear\", C=C, decision_function_shape='ovo')\n",
    "model2 = svm.LinearSVC(C=C, max_iter=10000)\n",
    "model3 = svm.SVC(kernel=\"rbf\", gamma=0.7, C=C, decision_function_shape='ovo')\n",
    "model4 = svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\",\n",
    "                 C=C, decision_function_shape='ovo')\n",
    "model5 = svm.SVC(kernel=\"sigmoid\", gamma=\"auto\", C=C, decision_function_shape='ovo')\n",
    "\n",
    "# random forest\n",
    "model6 = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "model7 = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=0)\n",
    "model8 = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=0)\n",
    "model9 = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "\n",
    "excludeList = ['SIZE_HA_BIN', 'YEAR', 'DAY', 'DAYW']\n",
    "dfFeatures = dfTrainScaled.drop(excludeList, axis=1)\n",
    "dfLabel = dfTrainScaled['SIZE_HA_BIN']\n",
    "\n",
    "model1 = model1.fit(dfFeatures, dfLabel)\n",
    "model2 = model2.fit(dfFeatures, dfLabel)\n",
    "model3 = model3.fit(dfFeatures, dfLabel)\n",
    "model4 = model4.fit(dfFeatures, dfLabel)\n",
    "model5 = model5.fit(dfFeatures, dfLabel)\n",
    "model6 = model6.fit(dfFeatures, dfLabel)\n",
    "model7 = model7.fit(dfFeatures, dfLabel)\n",
    "model8 = model8.fit(dfFeatures, dfLabel)\n",
    "model9 = model9.fit(dfFeatures, dfLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     13.0\n",
       "mean     585.4\n",
       "std     1167.6\n",
       "min        0.2\n",
       "25%       14.8\n",
       "50%       23.4\n",
       "75%      383.4\n",
       "max     3997.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfMeans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestScaled = pd.DataFrame()\n",
    "for colNum in range(0, len(dfTest.columns)):\n",
    "    dfTestScaled[dfTest.columns[colNum]] = (dfTest[dfTest.columns[colNum]] - dfMeans[colNum]) / dfStdevs[colNum]\n",
    "    # dfTestScaled = (dfTest - dfMeans) / dfStdevs\n",
    "\n",
    "# shift the wole test set to be positive\n",
    "# dfTestScaled = dfTestScaled + 3\n",
    "\n",
    "# add year column back\n",
    "dfTestScaled['YEAR'] = dfTest['YEAR']\n",
    "dfTestScaled['SIZE_HA_BIN'] = dfTrim[dfTrim['YEAR'].isin(\n",
    "    [2017, 2018, 2019, 2020])]['SIZE_HA_BIN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        YEAR  MONTH  DAY  ELEVATIONM  DIST_TO_WATER  DAYW  MAXTEMP  \\\n",
      "count   82.0   82.0 82.0        82.0           82.0  82.0     82.0   \n",
      "mean  2017.7    0.6 -0.1        -0.0           -0.2  -0.1      0.2   \n",
      "std      0.9    1.1  0.9         1.2            0.8   0.9      0.9   \n",
      "min   2017.0   -2.6 -1.5        -0.9           -0.9  -1.5     -2.8   \n",
      "25%   2017.0    0.5 -0.7        -0.6           -0.7  -0.7     -0.1   \n",
      "50%   2017.0    0.5  0.1        -0.5           -0.5   0.1      0.3   \n",
      "75%   2018.0    1.5  0.7        -0.3            0.3   0.7      0.8   \n",
      "max   2020.0    3.6  1.5         6.4            2.9   1.5      1.7   \n",
      "\n",
      "       MEANHUMIDITY  MEANWINDSPEED  MAXWINDSPEED  RAIN  LONGBIN  LATBIN  \n",
      "count          82.0           82.0          82.0  82.0     82.0    82.0  \n",
      "mean            0.1           -0.2          -0.4  -0.1      0.3     0.2  \n",
      "std             0.8            1.1           1.0   1.0      1.1     1.3  \n",
      "min            -2.5           -1.7          -2.0  -0.5     -3.5    -3.5  \n",
      "25%            -0.4           -1.1          -1.1  -0.5     -0.2    -0.3  \n",
      "50%             0.2           -0.5          -0.7  -0.5      0.7     0.6  \n",
      "75%             0.8            0.4           0.2  -0.5      1.2     1.1  \n",
      "max             1.7            2.5           2.6   2.1      2.0     1.9  \n"
     ]
    }
   ],
   "source": [
    "print(dfTestScaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestFeatures = dfTestScaled.drop(excludeList, axis=1)\n",
    "\n",
    "# predict on test data\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN1'] = model1.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN2'] = model2.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN3'] = model3.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN4'] = model4.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN5'] = model5.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN6'] = model6.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN7'] = model7.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN8'] = model8.predict(dfTestFeatures)\n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN9'] = model9.predict(dfTestFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show roc curve\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show precision recall curve\n",
    "def plot_precision_recall_curve(precision, recall, label=None):\n",
    "    plt.plot(recall, precision, linewidth=2, label=label)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show accuracy, precision, recall, f1 score\n",
    "# def show_metrics(y_test, y_pred, labels):\n",
    "#     print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "#     print(\"Precision: \", precision_score(\n",
    "#         y_test, y_pred, labels=labels, average='micro'))\n",
    "#     print(\"Recall: \", recall_score(\n",
    "#         y_test, y_pred, labels=labels, average='micro'))\n",
    "#     print(\"F1 Score: \", f1_score(y_test, y_pred, labels=labels, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show accuracy, precision, recall, f1 score\n",
    "def show_metrics(y_test, y_pred):\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1 Score: \", f1_score(y_test, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  1\n",
      "Accuracy:  0.15853658536585366\n",
      "Precision:  0.20147058823529412\n",
      "Recall:  0.15900974025974027\n",
      "F1 Score:  0.1449074074074074\n",
      "Model  2\n",
      "Accuracy:  0.15853658536585366\n",
      "Precision:  0.13054511278195488\n",
      "Recall:  0.17056277056277053\n",
      "F1 Score:  0.14271739130434782\n",
      "Model  3\n",
      "Accuracy:  0.24390243902439024\n",
      "Precision:  0.21024080086580083\n",
      "Recall:  0.2533820346320347\n",
      "F1 Score:  0.19578451191354412\n",
      "Model  4\n",
      "Accuracy:  0.23170731707317074\n",
      "Precision:  0.23027210884353738\n",
      "Recall:  0.23725649350649347\n",
      "F1 Score:  0.19481596869147982\n",
      "Model  5\n",
      "Accuracy:  0.3048780487804878\n",
      "Precision:  0.33152958152958156\n",
      "Recall:  0.3046266233766234\n",
      "F1 Score:  0.2738095238095238\n",
      "Model  6\n",
      "Accuracy:  0.21951219512195122\n",
      "Precision:  0.15495770676691728\n",
      "Recall:  0.22738095238095235\n",
      "F1 Score:  0.17871883511418396\n",
      "Model  7\n",
      "Accuracy:  0.2804878048780488\n",
      "Precision:  0.27598846253564946\n",
      "Recall:  0.2772727272727273\n",
      "F1 Score:  0.2703503737823756\n",
      "Model  8\n",
      "Accuracy:  0.25609756097560976\n",
      "Precision:  0.2505847953216374\n",
      "Recall:  0.2534632034632035\n",
      "F1 Score:  0.24604072398190047\n",
      "Model  9\n",
      "Accuracy:  0.25609756097560976\n",
      "Precision:  0.2505847953216374\n",
      "Recall:  0.2534632034632035\n",
      "F1 Score:  0.24604072398190047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "labels = ['L', 'M', 'H', 'E']\n",
    "# show metrics for each model\n",
    "for i in range(1, 10):\n",
    "    print(\"Model \", i)\n",
    "    show_metrics(dfTestScaled['SIZE_HA_BIN'],\n",
    "                 dfTestScaled['PREDICTED_SIZE_HA_BIN' + str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestScaled.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3014ezH3C7jT"
   },
   "source": [
    "## Define functions that build and train a model\n",
    "\n",
    "The following code defines two functions:\n",
    "\n",
    "  * `build_model(my_learning_rate)`, which builds a randomly-initialized model.\n",
    "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass. \n",
    "\n",
    "Since you don't need to understand model building code right now, we've hidden this code cell.  You may optionally double-click the following headline to see the code that builds and trains a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "pedD5GhlDC-y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the build_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that TensorFlow can efficiently\n",
    "  # execute. Configure training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model        \n",
    "\n",
    "\n",
    "def train_model(model, df, feature, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Feed the model the feature and the label.\n",
    "  # The model will train for the specified number of epochs. \n",
    "  history = model.fit(x=df[feature],\n",
    "                      y=df[label],\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Isolate the error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # To track the progression of training, we're going to take a snapshot\n",
    "  # of the model's root mean squared error at each epoch. \n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return trained_weight, trained_bias, epochs, rmse\n",
    "\n",
    "print(\"Defined the build_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak_TMAzGOIFq"
   },
   "source": [
    "## Define plotting functions\n",
    "\n",
    "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) functions create the following plots:\n",
    "\n",
    "*  a scatter plot of the feature vs. the label, and a line showing the output of the trained model\n",
    "*  a loss curve\n",
    "\n",
    "You may optionally double-click the headline to see the matplotlib code, but note that writing matplotlib code is not an important part of learning ML programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "id": "QF0BFRXTOeR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_model and plot_the_loss_curve functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(feature)\n",
    "  plt.ylabel(label)\n",
    "\n",
    "  # Create a scatter plot from 200 random points of the dataset.\n",
    "  random_examples = dfTrainScaled.sample(n=dfTrainScaled.shape[0])\n",
    "  plt.scatter(random_examples[feature], random_examples[label])\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = 6\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "  plt.show()  \n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-IXYVfvM4gD"
   },
   "source": [
    "## Call the model functions\n",
    "\n",
    "An important part of machine learning is determining which [features](https://developers.google.com/machine-learning/glossary/#feature) correlate with the [label](https://developers.google.com/machine-learning/glossary/#label). For example, real-life home-value prediction models typically rely on hundreds of features and synthetic features. However, this model relies on only one feature. For now, you'll arbitrarily use `total_rooms` as that feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'28daySumMaxTemp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3803\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '28daySumMaxTemp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_327632\\2957009584.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Invoke the functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmy_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m weight, bias, epochs, rmse = train_model(my_model, dfTrainScaled, \n\u001b[0m\u001b[0;32m     19\u001b[0m                                          \u001b[0mmy_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                          epochs, batch_size)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_327632\\281240721.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, df, feature, label, epochs, batch_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;31m# Feed the model the feature and the label.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;31m# The model will train for the specified number of epochs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m   history = model.fit(x=df[feature],\n\u001b[0m\u001b[0;32m     28\u001b[0m                       \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3805\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3806\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3805\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3806\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '28daySumMaxTemp'"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.02\n",
    "epochs = 30\n",
    "batch_size = 3\n",
    "\n",
    "# Specify the feature and the label.\n",
    "my_feature = \"28daySumMaxTemp\"  # the total number of rooms on a specific city block.\n",
    "my_label=\"SIZE_HA\" # the median value of a house on a specific city block.\n",
    "#my_label=\"size_ha_bin\"\n",
    "# That is, you're going to create a model that predicts house value based \n",
    "# solely on total_rooms.  \n",
    "\n",
    "# Discard any pre-existing version of the model.\n",
    "my_model = None\n",
    "\n",
    "# Invoke the functions.\n",
    "my_model = build_model(learning_rate)\n",
    "weight, bias, epochs, rmse = train_model(my_model, dfTrainScaled, \n",
    "                                         my_feature, my_label,\n",
    "                                         epochs, batch_size)\n",
    "\n",
    "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
    "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
    "\n",
    "plot_the_model(weight, bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xNqWWos_zyk"
   },
   "source": [
    "## Use the model to make predictions\n",
    "\n",
    "You can use the trained model to make predictions. In practice, [you should make predictions on examples that are not used in training](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). However, for this exercise, you'll just work with a subset of the same training dataset. A later Colab exercise will explore ways to make predictions on examples not used in training.\n",
    "\n",
    "First, run the following code to define the house prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH63BmncAcab"
   },
   "outputs": [],
   "source": [
    "def predict_house_values(n, feature, label):\n",
    "  \"\"\"Predict house values based on a feature.\"\"\"\n",
    "\n",
    "  batch = dfTrainScaled[feature][200:200 + n]\n",
    "  predicted_values = my_model.predict_on_batch(x=batch)\n",
    "\n",
    "  print(\"feature   label          predicted\")\n",
    "  print(\"  value   value          value\")\n",
    "  print(\"          in thousand$   in thousand$\")\n",
    "  print(\"--------------------------------------\")\n",
    "  for i in range(n):\n",
    "    print (\"%5.0f %6.0f %15.0f\" % (dfTrain[feature][400+i], dfTrain[label][400+i], predicted_values[i][0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_0DGBt0Kz_N"
   },
   "outputs": [],
   "source": [
    "predict_house_values(10, my_feature, my_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/Cast' defined at (most recent call last):\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Red\\AppData\\Local\\Temp\\ipykernel_293576\\1900820070.py\", line 12, in <cell line: 12>\n      model.fit(dfFeatures, dfLabel, epochs=100)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 5583, in sparse_categorical_crossentropy\n      target = cast(target, \"int64\")\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 2295, in cast\n      return tf.cast(x, dtype)\nNode: 'sparse_categorical_crossentropy/Cast'\nCast string to int64 is not supported\n\t [[{{node sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_981]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_293576\\1900820070.py\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/Cast' defined at (most recent call last):\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Red\\AppData\\Local\\Temp\\ipykernel_293576\\1900820070.py\", line 12, in <cell line: 12>\n      model.fit(dfFeatures, dfLabel, epochs=100)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 5583, in sparse_categorical_crossentropy\n      target = cast(target, \"int64\")\n    File \"f:\\Programs\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 2295, in cast\n      return tf.cast(x, dtype)\nNode: 'sparse_categorical_crossentropy/Cast'\nCast string to int64 is not supported\n\t [[{{node sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_981]"
     ]
    }
   ],
   "source": [
    "# create nn model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# train model\n",
    "model.fit(dfFeatures, dfLabel, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data  \n",
    "dfTestScaled['PREDICTED_SIZE_HA_BIN10'] = model.predict_classes(dfTestFeatures)\n",
    "\n",
    "# show metrics for each model\n",
    "print(\"Model 10\")\n",
    "show_metrics(dfTestScaled['SIZE_HA_BIN'], dfTestScaled['PREDICTED_SIZE_HA_BIN10'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7e3ad1fe0a4a5293cd6ca311ffca45c667fb34d948e973384e8d54f12a93f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
