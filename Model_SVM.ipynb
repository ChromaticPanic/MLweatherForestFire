{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv\n",
    "# %pip install seaborn\n",
    "# %pip install tensorflow_data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 01:57:00.275052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 01:57:00.400141: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeos as pg\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_data_validation as tfdv\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "# from datetime import datetime\n",
    "# from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines adjust the granularity of reporting.\n",
    "#pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "os.chdir('/tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPDfromPD(df: pd.DataFrame, geomCol: str, crs: str = \"EPSG:3978\") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert a pandas dataframe to a geopandas dataframe\n",
    "    :param df: pandas dataframe\n",
    "    :param geomCol: name of the geometry column\n",
    "    :param crs: coordinate reference system\n",
    "    :return: geopandas dataframe\n",
    "    \"\"\"\n",
    "    if 'geom' in df.columns:\n",
    "        df.rename(columns={'geom': 'geometry'}, inplace=True)\n",
    "\n",
    "    df[geomCol] = df[geomCol].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geomCol, crs=crs)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireWeatherTable = \"Data/FinalFeature.csv\"\n",
    "dfFireWeather = pd.read_csv(fireWeatherTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEval = dfFireWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our random selection, run once\n",
    "# randomTrain = \"RandomTrain\"\n",
    "# dfTrain.to_sql(randomTrain, db_push_con, if_exists='replace', index=False)\n",
    "\n",
    "# randomTest = \"RandomTest\"\n",
    "# dfTest.to_sql(randomTest, db_push_con, if_exists='replace', index=False)\n",
    "\n",
    "# randomValidate = \"RandomValidate\"\n",
    "# dfValidate.to_sql(randomValidate, db_push_con, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn import svm\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrim = dfEval.copy(deep=True)\n",
    "\n",
    "#edit this\n",
    "dfTrim = dfTrim.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 6 years from 2010-2019 for training\n",
    "dfTrain = dfTrim[dfTrim['YEAR'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016])]\n",
    "dfTest = dfTrim[dfTrim['YEAR'].isin([2017, 2018, 2019, 2020])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # regularize y values using z score\n",
    "    df = (df - df.mean()) / df.std()\n",
    "    # set max value to 3 zscore\n",
    "    df[df > 3] = 3\n",
    "    # set min value to -3 zscore\n",
    "    df[df < -3] = -3\n",
    "\n",
    "    # shift the wole train set to be positive\n",
    "    df = df + 3\n",
    "\n",
    "    # return dataframe list of means and stds\n",
    "    return df, df.mean(), df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainScaled, dfMeans, dfStdevs = scaleData(dfTrain.copy(deep=True))\n",
    "dfTrainScaled['YEAR'] = dfTrain['YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dfTrainScaled.count())\n",
    "print(dfTrainScaled.isna().sum().sum())\n",
    "print(dfTrainScaled.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrainScaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1  # SVM regularization parameter\n",
    "\n",
    "model1 = svm.SVC(kernel=\"linear\", C=C, decision_function_shape='ovo'),\n",
    "model2 = svm.LinearSVC(C=C, max_iter=10000),\n",
    "model3 = svm.SVC(kernel=\"rbf\", gamma=0.7, C=C, decision_function_shape='ovo'),\n",
    "model4 = svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C, decision_function_shape='ovo'),\n",
    "\n",
    "dfFeatures = dfTrainScaled.drop(['size_ha_bin', 'YEAR'], axis=1)\n",
    "dfLabel = dfTrainScaled['size_ha_bin']\n",
    "\n",
    "model1 = model1.fit(dfFeatures, dfLabel)\n",
    "model2 = model2.fit(dfFeatures, dfLabel)\n",
    "model3 = model3.fit(dfFeatures, dfLabel)\n",
    "model4 = model4.fit(dfFeatures, dfLabel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
