{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 00:59:40.474485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-17 00:59:40.583713: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeos as pg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sqlalchemy as sq\n",
    "import calendar\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from DataService import DataService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir('/tf')\n",
    "PGUSER = os.getenv('POSTGRES_USER')\n",
    "PGPW = os.getenv('POSTGRES_PW')\n",
    "PGDB = os.getenv('POSTGRES_DB')\n",
    "NULLFLAG = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "pullService = DataService(PGDB, PGUSER, PGPW)\n",
    "db_pull_con = pullService.connect()\n",
    "\n",
    "pushService = DataService(PGDB, PGUSER, PGPW)\n",
    "db_push_con = pushService.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_iter(year, month):\n",
    "    \"\"\"Iterate over days in a month.\"\"\"\n",
    "    for i in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryDaily(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns new df with min max mean for each numeric column\"\"\"\n",
    "    \n",
    "    result = df.groupby(['ClimateID', 'ProvinceCode', 'Year', 'Month', 'Day'], as_index=False).agg({\n",
    "                                                                                                    'Temp':[('MeanTemp', 'mean'), ('MinTemp', 'min'), ('MaxTemp', 'max')],\n",
    "                                                                                                  'DewPointTemp':[('MeanDewPoint', 'mean'), ('MinDewPoint', 'min'), ('MaxDewPoint', 'max')],\n",
    "                                                                                                  'RelativeHumidity':[('MeanHumidity', 'mean'), ('MinHumidity', 'min'), ('MaxHumidity', 'max')],\n",
    "                                                                                                  'StationPressure':[('MeanPressure', 'mean'), ('MinPressure', 'min'), ('MaxPressure', 'max')],\n",
    "                                                                                                  'WindSpeed':[('MeanWindSpeed', 'mean'), ('MinWindSpeed', 'min'), ('MaxWindSpeed', 'max')],\n",
    "                                                                                                  'WindChill':[('MeanWindChill', 'mean'), ('MinWindChill', 'min'), ('MaxWindChill', 'max')],\n",
    "                                                                                                  'PrecipAmount':[('TotalPrecip', 'sum')], 'WindDirection':[('MeanWindDirection', 'mean')]\n",
    "                                                                                                  })\n",
    "    # result.drop(columns=['Hour'], inplace=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryMonthly(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns new df with min max mean for each numeric column\"\"\"\n",
    "    \n",
    "    result = df.groupby(['ClimateID', 'ProvinceCode', 'Year', 'Month'], as_index=False).agg({\n",
    "                                                                                              'Temp':[('MeanTemp', 'mean'), ('MinTemp', 'min'), ('MaxTemp', 'max')],\n",
    "                                                                                            'DewPointTemp':[('MeanDewPoint', 'mean'), ('MinDewPoint', 'min'), ('MaxDewPoint', 'max')],\n",
    "                                                                                            'RelativeHumidity':[('MeanHumidity', 'mean'), ('MinHumidity', 'min'), ('MaxHumidity', 'max')],\n",
    "                                                                                            'StationPressure':[('MeanPressure', 'mean'), ('MinPressure', 'min'), ('MaxPressure', 'max')],\n",
    "                                                                                            'WindSpeed':[('MeanWindSpeed', 'mean'), ('MinWindSpeed', 'min'), ('MaxWindSpeed', 'max')],\n",
    "                                                                                            'WindChill':[('MeanWindChill', 'mean'), ('MinWindChill', 'min'), ('MaxWindChill', 'max')],\n",
    "                                                                                            'PrecipAmount':[('TotalPrecip', 'sum')], 'WindDirection':[('MeanWindDirection', 'mean')]\n",
    "                                                                                            })\n",
    "    # result.drop(columns=['Hour'], inplace=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanup(id: str, year: int, month: int, day: int, srcTable: str, destTable: str) -> None:\n",
    "def cleanup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.astype({'ClimateID': 'str', \n",
    "            'ProvinceCode': 'str', \n",
    "                    'Year': 'int', \n",
    "                    'Month': 'int', \n",
    "                    'Day': 'int', \n",
    "                    'Hour': 'int', \n",
    "                    'Temp': 'float', \n",
    "                    'DewPointTemp': 'float', \n",
    "                    'PrecipAmount': 'float', \n",
    "                    'RelativeHumidity': 'float', \n",
    "                    'StationPressure': 'float', \n",
    "                    'WindChill': 'float', \n",
    "                    'WindDirection': 'float', \n",
    "                    'WindSpeed': 'float'}, copy=False)\n",
    "\n",
    "    # Replace NULLFLAG values with mean for each column\n",
    "    df = df.replace(NULLFLAG, np.nan)\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "climateIdTable = \"WeatherDataHourlyTwentyYear\"\n",
    "\n",
    "# first get distinct list of stations\n",
    "query = \"SELECT DISTINCT \\\"ClimateID\\\" FROM public.\\\"{}\\\";\".format(climateIdTable)\n",
    "dfIDs = pd.read_sql(query, db_pull_con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTable = \"WeatherDataHourlyTwentyYear\"\n",
    "destTable = \"WeatherDataHourlyAggDaily\"\n",
    "# Replace NULLFLAG values with mean for each column\n",
    "for id in dfIDs['ClimateID']:\n",
    "    clear_output(wait=False)\n",
    "    print(\"Processing ClimateID: {}\".format(id))\n",
    "\n",
    "    # Iterate through days from 2009 to 2022\n",
    "    year = 2010\n",
    "    month = 1\n",
    "    for year in range(2000, 2022):\n",
    "        # first we check if the db has data for the year\n",
    "        query = \"SELECT * FROM public.\\\"{}\\\" WHERE \\\"ClimateID\\\" like '{}' AND \\\"Year\\\" = {};\".format(srcTable, id, year)\n",
    "        dfYear = pd.read_sql(query, db_pull_con)\n",
    "        if dfYear.empty:\n",
    "            continue\n",
    "        \n",
    "        monthList = dfYear['Month'].unique()\n",
    "        for month in monthList:\n",
    "            dfMonth = dfYear[dfYear['Month'] == month]\n",
    "            \n",
    "            dayList = dfMonth['Day'].unique()\n",
    "            for day in dayList:\n",
    "                dfDay = dfMonth[dfMonth['Day'] == day]\n",
    "               \n",
    "                dfClean = cleanup(dfDay)\n",
    "                \n",
    "                # Get df with min max mean for each numeric column\n",
    "                dfSummary = summaryDaily(dfClean)\n",
    "\n",
    "                # Update the database\n",
    "                dfSummary.to_sql(destTable, db_push_con, if_exists='append', index=False)\n",
    "\n",
    "                print(\"Processed ClimateID: {} for {}\".format(id, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTable = \"WeatherDataHourlyAggDaily\"\n",
    "dailyAgg = pd.read_sql(\"SELECT * FROM public.\\\"{}\\\";\".format(srcTable), db_pull_con)\n",
    "dailyAgg.to_csv(\"WeatherDataHourlyAggDaily.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "srcTable = \"WeatherDataHourly\"\n",
    "destTable = \"WeatherDataHourlyAggMonthly\"\n",
    "# Replace NULLFLAG values with mean for each column\n",
    "for id in dfIDs['ClimateID']:\n",
    "    clear_output(wait=False)\n",
    "    print(\"Processing ClimateID: {}\".format(id))\n",
    "\n",
    "    # Iterate through days from 2009 to 2022\n",
    "    year = 2010\n",
    "    month = 1\n",
    "    for year in range(2010, 2022):\n",
    "        # first we check if the db has data for the year\n",
    "        query = \"SELECT * FROM public.\\\"{}\\\" WHERE \\\"ClimateID\\\" like '{}' AND \\\"Year\\\" = {};\".format(srcTable, id, year)\n",
    "        dfYear = pd.read_sql(query, db_pull_con)\n",
    "        if dfYear.empty:\n",
    "            continue\n",
    "        \n",
    "        dfClean = cleanup(id, year, srcTable, destTable)\n",
    "\n",
    "         # Get df with min max mean for each numeric column\n",
    "        dfSummary = summaryMonthly(dfClean)\n",
    "\n",
    "        # Update the database\n",
    "        dfSummary.to_sql(destTable, db_push_con, if_exists='append', index=False)\n",
    "\n",
    "        print(\"Processed ClimateID: {} for {}\".format(id, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load WeatherDataHourlyAggDaily\n",
    "srcTable = \"WeatherDataHourlyAggDaily\"\n",
    "destTable = \"WeatherDataHourlyAggDaily\"\n",
    "query = \"SELECT * FROM public.\\\"{}\\\";\".format(srcTable)\n",
    "df = pd.read_sql(query, db_pull_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill utc column using year, month, day\n",
    "df['utc'] = df.apply(lambda row: int(datetime(row['Year'], row['Month'], row['Day']).timestamp()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(destTable, db_push_con, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FireWaterElev\n",
    "srcTable = \"FireWaterElev\"\n",
    "destTable = \"FireWaterElev\"\n",
    "query = \"SELECT * FROM public.\\\"{}\\\";\".format(srcTable)\n",
    "df = pd.read_sql(query, db_pull_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill utc column using year, month, day\n",
    "df['utc'] = df.apply(lambda row: int(datetime(row['YEAR'], row['MONTH'], row['DAY']).timestamp()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/io/sql.py:1781: SAWarning: Did not recognize type 'geometry' of column 'geom'\n",
      "  self.meta.reflect(bind=self.connectable, only=[table_name], schema=schema)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(destTable, db_push_con, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
